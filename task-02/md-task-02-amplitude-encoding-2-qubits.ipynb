{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ccfe335-e640-4b2c-b933-cba2bd38fab7",
   "metadata": {},
   "source": [
    "# Amplitude Encoding - 2 qubits - using qml.AmplitudeEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2481de7-3eb0-4d54-ae30-a6b8b55637bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3fb9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer\n",
    "\n",
    "import sys\n",
    "from math import sqrt, pi\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "# supress a warning that is not useful here\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac746261",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration\n",
    "\n",
    "Second and third columns looks better as decimal logarithms. Also it seems they are highly corelated and eventually can be compressed to one feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "407ceefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data standardised:\n",
      " [[ 2.78926e+03  3.00000e+00  1.00000e+00  2.00000e+01 -1.00000e+00]\n",
      " [ 4.04001e+03  6.00000e+00  0.00000e+00  1.00000e+00  1.00000e+00]\n",
      " [ 2.93120e+03  4.00000e+00  4.00000e+00  4.00000e+01  1.00000e+00]\n",
      " ...\n",
      " [ 4.18281e+03  0.00000e+00  0.00000e+00  6.50000e+01 -1.00000e+00]\n",
      " [ 3.11375e+03  4.00000e+00  2.00000e+00  1.00000e+00  1.00000e+00]\n",
      " [ 4.56757e+03  4.00000e+00  5.00000e+00  9.00000e+01  1.00000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StandardScaler(with_mean=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data normalized:\n",
      " [[0.78842549 0.50347726 0.20019498 0.29123507]\n",
      " [0.7500201  0.66134589 0.         0.00956384]\n",
      " [0.56936298 0.46130771 0.55028199 0.40026331]\n",
      " ...\n",
      " [0.78066018 0.         0.         0.62495574]\n",
      " [0.74765323 0.57024753 0.34011673 0.01236968]\n",
      " [0.5870369  0.30522994 0.45512608 0.5958881 ]]\n"
     ]
    }
   ],
   "source": [
    "## #################################################################################\n",
    "# globals\n",
    "\n",
    "path_train = '/_jupyter/QC/QOSF-challenge-md-2022/task-02/mock_train_set.csv'\n",
    "path_test = '/_jupyter/QC/QOSF-challenge-md-2022/task-02/mock_test_set.csv'\n",
    "\n",
    "num_qubits = 2\n",
    "num_layers = 4\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
    "\n",
    "df = pd.read_csv(path_train)\n",
    "df_c = df.copy(deep=True)\n",
    "df['1'] = np.log10(df_c['1'])\n",
    "df['2'] = np.log10(df_c['2'])\n",
    "\n",
    "f = lambda x: -1.0 if x==0 else 1.0\n",
    "df['4'] = df_c['4'].map(f)\n",
    "\n",
    "# npdf = df2.to_numpy()\n",
    "npdf = df.to_numpy()\n",
    "data = np.array(npdf)\n",
    "\n",
    "print(\"Train data standardised:\\n\", data)\n",
    "\n",
    "X = data[:, 0:4]\n",
    "Y = data[:, -1]\n",
    "\n",
    "# scale the data using sklearn StandardScaler\n",
    "std_slc = StandardScaler(with_mean=False)\n",
    "std_slc.fit(X)\n",
    "X_std = std_slc.transform(X)\n",
    "\n",
    "# normalize data using sklearn StandardScaler\n",
    "normalizer = Normalizer().fit(X_std)  # fit does nothing.\n",
    "X_norm = normalizer.transform(X_std)\n",
    "\n",
    "\n",
    "# features will be applitudes vector\n",
    "features = np.array(X_norm, requires_grad=False)\n",
    "print(\"Train data normalized:\\n\", features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa8c3d",
   "metadata": {},
   "source": [
    "## Data Encoding, Circuit Preparation and Cost Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c55af916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################################################################\n",
    "# layer circuit - this is where the circuit learns\n",
    "def layer(W):\n",
    "    qml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)\n",
    "    qml.Rot(W[1, 0], W[1, 1], W[1, 2], wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "\n",
    "# ###################################################################\n",
    "# the circuit - where the action happens\n",
    "@qml.qnode(dev)\n",
    "def circuit(weights, data):\n",
    "\n",
    "    qml.AmplitudeEmbedding(features=data, wires=range(num_qubits))\n",
    "\n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "draw_flag = 0\n",
    "\n",
    "# ###############################################################################\n",
    "# variational classifier\n",
    "# this will be called on each optimization step by the cost evaluation\n",
    "def variational_classifier(weights, bias, data):\n",
    "    \n",
    "    global draw_flag\n",
    "\n",
    "    if draw_flag:\n",
    "        draw_flag=0\n",
    "        # qml.draw\n",
    "\n",
    "    return circuit(weights, data) + bias\n",
    "\n",
    "# ###############################################################################\n",
    "# standard square loss\n",
    "def square_loss(labels, predictions):\n",
    "    \n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "# ###############################################################################\n",
    "# goal: maximize accuracy\n",
    "def accuracy(labels, predictions):\n",
    "\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l - p) < 1e-5:\n",
    "            loss = loss + 1\n",
    "    loss = loss / len(labels)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# ###############################################################################\n",
    "# goal: maximize accuracy\n",
    "def cost(weights, bias, features, labels):\n",
    "    \n",
    "    predictions = [variational_classifier(weights, bias, f) for f in features]\n",
    "    return square_loss(labels, predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1a0e4e",
   "metadata": {},
   "source": [
    "## Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a59fb6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(with_mean=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "num_data = len(Y)\n",
    "\n",
    "num_train = int(0.75 * num_data)\n",
    "index = np.random.permutation(range(num_data))\n",
    "\n",
    "feats_train = features[index[:num_train]]\n",
    "Y_train = Y[index[:num_train]]\n",
    "\n",
    "feats_val = features[index[num_train:]]\n",
    "Y_val = Y[index[num_train:]]\n",
    "\n",
    "# We need these later for plotting\n",
    "X_train = X[index[:num_train]]\n",
    "X_val = X[index[num_train:]]\n",
    "\n",
    "# ######################################################################\n",
    "# Load the test dataset and apply same transformations as we did with the train dataset.\n",
    "df_test = pd.read_csv(path_test)\n",
    "df_test_c = df_test.copy(deep=True)\n",
    "df_test['1'] = np.log10(df_test_c['1'])\n",
    "df_test['2'] = np.log10(df_test_c['2'])\n",
    "\n",
    "f = lambda x: -1.0 if x==0 else 1.0\n",
    "df_test['4'] = df_test_c['4'].map(f)\n",
    "data_test = df_test.to_numpy()\n",
    "\n",
    "X_test_ini = data_test[:, 0:4]\n",
    "Y_test = data_test[:, -1]\n",
    "\n",
    "# scale data using sklearn StandardScaler\n",
    "std_slc = StandardScaler(with_mean=False)\n",
    "std_slc.fit(X_test_ini)\n",
    "X_test_std = std_slc.transform(X_test_ini)\n",
    "\n",
    "# normalize data using sklearn Normalizer\n",
    "normalizer = Normalizer().fit(X_test_std)  # fit does nothing.\n",
    "X_test_norm = normalizer.transform(X_test_std)\n",
    "\n",
    "# convert to a pennylane numpy array\n",
    "X_test = np.array(X_test_norm, requires_grad=False)\n",
    "\n",
    "\n",
    "# ######################################################################\n",
    "# accuracy for test dtaset\n",
    "def test_accuracy(weights, bias):\n",
    "    # apply the variational clasifier circuit on test dataset\n",
    "    # using the learned weights\n",
    "    predictions_test = [np.sign(variational_classifier(weights, bias, f)) for f in X_test]\n",
    "\n",
    "    return accuracy(Y_test, predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af08f90b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "544af8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 1.1970236 | Acc train: 0.5288889 | Acc validation: 0.5066667 | Acc test: 0.4333333 \n",
      "Iter:     2 | Cost: 1.1949626 | Acc train: 0.5288889 | Acc validation: 0.5066667 | Acc test: 0.4333333 \n",
      "Iter:     3 | Cost: 1.1951781 | Acc train: 0.5288889 | Acc validation: 0.4933333 | Acc test: 0.4250000 \n",
      "Iter:     4 | Cost: 1.1949684 | Acc train: 0.5200000 | Acc validation: 0.5333333 | Acc test: 0.4083333 \n",
      "Iter:     5 | Cost: 1.2056649 | Acc train: 0.5111111 | Acc validation: 0.4933333 | Acc test: 0.3916667 \n",
      "Iter:     6 | Cost: 1.2253642 | Acc train: 0.4888889 | Acc validation: 0.5066667 | Acc test: 0.3666667 \n",
      "Iter:     7 | Cost: 1.2349040 | Acc train: 0.4933333 | Acc validation: 0.5333333 | Acc test: 0.3250000 \n",
      "Iter:     8 | Cost: 1.2546892 | Acc train: 0.4977778 | Acc validation: 0.4933333 | Acc test: 0.3333333 \n",
      "Iter:     9 | Cost: 1.2799436 | Acc train: 0.4888889 | Acc validation: 0.4933333 | Acc test: 0.3500000 \n",
      "Iter:    10 | Cost: 1.3077180 | Acc train: 0.4933333 | Acc validation: 0.4933333 | Acc test: 0.3416667 \n",
      "Iter:    11 | Cost: 1.3059828 | Acc train: 0.4977778 | Acc validation: 0.4933333 | Acc test: 0.3416667 \n",
      "Iter:    12 | Cost: 1.2774929 | Acc train: 0.4933333 | Acc validation: 0.4933333 | Acc test: 0.3500000 \n",
      "Iter:    13 | Cost: 1.2362791 | Acc train: 0.4977778 | Acc validation: 0.4933333 | Acc test: 0.3416667 \n",
      "Iter:    14 | Cost: 1.2022030 | Acc train: 0.5022222 | Acc validation: 0.5333333 | Acc test: 0.3583333 \n",
      "Iter:    15 | Cost: 1.1625149 | Acc train: 0.5200000 | Acc validation: 0.5200000 | Acc test: 0.4083333 \n",
      "Iter:    16 | Cost: 1.1294733 | Acc train: 0.5466667 | Acc validation: 0.5600000 | Acc test: 0.4416667 \n",
      "Iter:    17 | Cost: 1.1067015 | Acc train: 0.5600000 | Acc validation: 0.5600000 | Acc test: 0.4416667 \n",
      "Iter:    18 | Cost: 1.0889679 | Acc train: 0.5733333 | Acc validation: 0.5600000 | Acc test: 0.4583333 \n",
      "Iter:    19 | Cost: 1.0718677 | Acc train: 0.5822222 | Acc validation: 0.5466667 | Acc test: 0.4833333 \n",
      "Iter:    20 | Cost: 1.0547172 | Acc train: 0.5866667 | Acc validation: 0.5733333 | Acc test: 0.5416667 \n",
      "Iter:    21 | Cost: 1.0406034 | Acc train: 0.6044444 | Acc validation: 0.5466667 | Acc test: 0.5833333 \n",
      "Iter:    22 | Cost: 1.0298418 | Acc train: 0.6222222 | Acc validation: 0.5200000 | Acc test: 0.5833333 \n",
      "Iter:    23 | Cost: 1.0181328 | Acc train: 0.6311111 | Acc validation: 0.5200000 | Acc test: 0.6083333 \n",
      "Iter:    24 | Cost: 1.0059499 | Acc train: 0.6355556 | Acc validation: 0.5200000 | Acc test: 0.6333333 \n",
      "Iter:    25 | Cost: 0.9900625 | Acc train: 0.6355556 | Acc validation: 0.5200000 | Acc test: 0.6500000 \n",
      "Iter:    26 | Cost: 0.9735126 | Acc train: 0.6355556 | Acc validation: 0.5200000 | Acc test: 0.6583333 \n",
      "Iter:    27 | Cost: 0.9537682 | Acc train: 0.6400000 | Acc validation: 0.5600000 | Acc test: 0.6666667 \n",
      "Iter:    28 | Cost: 0.9338266 | Acc train: 0.6444444 | Acc validation: 0.5866667 | Acc test: 0.6750000 \n",
      "Iter:    29 | Cost: 0.9091984 | Acc train: 0.6577778 | Acc validation: 0.5866667 | Acc test: 0.6833333 \n",
      "Iter:    30 | Cost: 0.8875148 | Acc train: 0.6666667 | Acc validation: 0.6133333 | Acc test: 0.6916667 \n",
      "Iter:    31 | Cost: 0.8611971 | Acc train: 0.6755556 | Acc validation: 0.6400000 | Acc test: 0.7083333 \n",
      "Iter:    32 | Cost: 0.8369756 | Acc train: 0.6800000 | Acc validation: 0.6533333 | Acc test: 0.7250000 \n",
      "Iter:    33 | Cost: 0.8168892 | Acc train: 0.6888889 | Acc validation: 0.6533333 | Acc test: 0.7416667 \n",
      "Iter:    34 | Cost: 0.7952369 | Acc train: 0.7200000 | Acc validation: 0.6933333 | Acc test: 0.7833333 \n",
      "Iter:    35 | Cost: 0.7703413 | Acc train: 0.7333333 | Acc validation: 0.7466667 | Acc test: 0.8166667 \n",
      "Iter:    36 | Cost: 0.7497114 | Acc train: 0.7511111 | Acc validation: 0.7600000 | Acc test: 0.8583333 \n",
      "Iter:    37 | Cost: 0.7347174 | Acc train: 0.7511111 | Acc validation: 0.7333333 | Acc test: 0.8916667 \n",
      "Iter:    38 | Cost: 0.7219452 | Acc train: 0.7511111 | Acc validation: 0.7333333 | Acc test: 0.9000000 \n",
      "Iter:    39 | Cost: 0.7109557 | Acc train: 0.7600000 | Acc validation: 0.7200000 | Acc test: 0.9000000 \n",
      "Iter:    40 | Cost: 0.7053394 | Acc train: 0.7688889 | Acc validation: 0.7466667 | Acc test: 0.9000000 \n",
      "Iter:    41 | Cost: 0.7016557 | Acc train: 0.7733333 | Acc validation: 0.7466667 | Acc test: 0.9083333 \n",
      "Iter:    42 | Cost: 0.6994751 | Acc train: 0.7777778 | Acc validation: 0.7333333 | Acc test: 0.9083333 \n",
      "Iter:    43 | Cost: 0.7002442 | Acc train: 0.7733333 | Acc validation: 0.7466667 | Acc test: 0.8916667 \n",
      "Iter:    44 | Cost: 0.6961878 | Acc train: 0.7644444 | Acc validation: 0.7600000 | Acc test: 0.8916667 \n",
      "Iter:    45 | Cost: 0.6902468 | Acc train: 0.7733333 | Acc validation: 0.7466667 | Acc test: 0.9000000 \n",
      "Iter:    46 | Cost: 0.6822973 | Acc train: 0.7733333 | Acc validation: 0.7600000 | Acc test: 0.9083333 \n",
      "Iter:    47 | Cost: 0.6751173 | Acc train: 0.7644444 | Acc validation: 0.7733333 | Acc test: 0.9083333 \n",
      "Iter:    48 | Cost: 0.6692432 | Acc train: 0.7644444 | Acc validation: 0.7733333 | Acc test: 0.9083333 \n",
      "Iter:    49 | Cost: 0.6592638 | Acc train: 0.7777778 | Acc validation: 0.7866667 | Acc test: 0.9250000 \n",
      "Iter:    50 | Cost: 0.6504535 | Acc train: 0.7911111 | Acc validation: 0.8000000 | Acc test: 0.9333333 \n",
      "\n",
      "\n",
      "time in sec. for 50 steps: 146.52725958824158\n",
      "\n",
      "bias:  -0.6437216759436238\n",
      "\n",
      "weights:\n",
      " [[[ 1.03161962e-02 -7.18829780e-01 -8.74773777e-03]\n",
      "  [-2.95954686e-03  2.37489944e-01  2.02445794e-02]]\n",
      "\n",
      " [[ 1.22211568e-02 -1.22596596e-01 -1.14368757e-02]\n",
      "  [ 1.04327003e-02 -4.34521230e-01 -1.26116228e-02]]\n",
      "\n",
      " [[ 1.26977775e-02 -7.99206211e-01  1.11394380e-02]\n",
      "  [ 3.79460001e-03  1.32190022e-01 -5.86643054e-03]]\n",
      "\n",
      " [[-8.41157869e-03 -1.46913590e-01 -8.03409664e-03]\n",
      "  [-6.89549778e-03 -4.55532504e-03  1.74791590e-04]]]\n"
     ]
    }
   ],
   "source": [
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)\n",
    "\n",
    "# # start with learned step\n",
    "# w = np.load('/_jupyter/QC/QOSF-challenge-md-2022/task-02/temp-data/variational_classifier/data/mock_train_numpy_wights_01.npy', allow_pickle=True)\n",
    "# weights_init = np.array(w, requires_grad=True)\n",
    "# bias_init = np.array(-0.483902119474, requires_grad=True)\n",
    "\n",
    "\n",
    "opt = NesterovMomentumOptimizer(0.01)\n",
    "# opt = NesterovMomentumOptimizer(0.1)\n",
    "# opt = NesterovMomentumOptimizer(0.1)\n",
    "# batch_size = 5\n",
    "# batch_size = 15\n",
    "# batch_size = 20\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "# train the variational classifier\n",
    "weights = weights_init\n",
    "bias = bias_init\n",
    "# for it in range(60):\n",
    "steps = 50\n",
    "\n",
    "toc = time.time()\n",
    "for it in range(steps):\n",
    "\n",
    "    # Update the weights by one optimizer step\n",
    "    batch_index = np.random.randint(0, num_train, (batch_size,))\n",
    "    feats_train_batch = feats_train[batch_index]\n",
    "    Y_train_batch = Y_train[batch_index]\n",
    "    weights, bias, _, _ = opt.step(cost, weights, bias, feats_train_batch, Y_train_batch)\n",
    "\n",
    "    # Compute predictions on train and validation set\n",
    "    predictions_train = [np.sign(variational_classifier(weights, bias, f)) for f in feats_train]\n",
    "    predictions_val = [np.sign(variational_classifier(weights, bias, f)) for f in feats_val]\n",
    "\n",
    "    # Compute accuracy on train and validation set\n",
    "    acc_train = accuracy(Y_train, predictions_train)\n",
    "    acc_val = accuracy(Y_val, predictions_val)\n",
    "    acc_test = test_accuracy(weights, bias)\n",
    "\n",
    "    print(\n",
    "        \"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} | Acc test: {:0.7f} \"\n",
    "        \"\".format(it + 1, cost(weights, bias, features, Y), acc_train, acc_val, acc_test)\n",
    "    )\n",
    "\n",
    "    if acc_train >= 0.93 and acc_val >= 0.93:\n",
    "        # early stop\n",
    "        break\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "print('\\n\\ntime in sec. for {} steps: {}'.format(it+1, tic-toc))\n",
    "print('\\nbias: ', bias)    \n",
    "print('\\nweights:\\n', weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f55127",
   "metadata": {},
   "source": [
    "## Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b98d9a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.9333\n"
     ]
    }
   ],
   "source": [
    "acc_test = test_accuracy(weights, bias)\n",
    "print('Accuracy on test data: {:0.4f}'.format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2441696",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17a310c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
